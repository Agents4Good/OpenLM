<div align="center">
  <a href="https://huggingface.co/ibm-granite" target="_blank">
    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-IBM%20Granite-ffc107?color=ffc107&logoColor=white" />
  </a>
</div>

<p align="center">
  <a href="https://arxiv.org/abs/2405.04324"><b>üìú Paper - Granite Models</b></a>
</p>

---
## 1. Introdu√ß√£o

Os modelos Granite representam uma fam√≠lia de modelos desenvolvidos pela **IBM**. Lan√ßados em 2024, os Granite s√£o otimizados para desempenho, efici√™ncia e sustentabilidade em aplica√ß√µes corporativas.

Conforme descrito no artigo ‚Äú**Granite Models: A Sustainable Foundation for Enterprise AI**‚Äù ([arXiv:2405.04324](https://arxiv.org/abs/2405.04324)),
os modelos utilizam t√©cnicas de treinamento avan√ßadas para maximizar efici√™ncia computacional.

Os Granite est√£o dispon√≠veis em diferentes vers√µes otimizadas para capacidade computacional e aplica√ß√µes empresariais espec√≠ficas:

| Tamanho do Modelo | Descri√ß√£o                                                                 |
|-------------------|---------------------------------------------------------------------------|
| Granite **3B**          | Vers√£o leve, para dispositivos de borda e aplica√ß√µes locais.                     |
| Granite **8B**            | Equil√≠brio entre desempenho e custo computacional.                          |
| Granite **20B**           | Otimizado para tarefas empresariais complexas.                                 |
| Granite **34B**           | Para cen√°rios de alta escalabilidade e integra√ß√£o empresarial.                   |

**Aten√ß√£o!**  
> Granite n√£o √© apenas um modelo espec√≠fico, mas uma fam√≠lia de modelos, cada um voltado para aplica√ß√µes empresariais distintas.

---
## 2. Arquitetura do Modelo

- **Granite Code Base**: modelos de funda√ß√£o b√°sicos para tarefas relacionadas a c√≥digo.  
- **Granite Code Instruct**: modelos ajustados para seguir instru√ß√µes, refinados com uma combina√ß√£o de commits do Git emparelhados com instru√ß√µes humanas e
conjuntos de dados de instru√ß√µes de c√≥digo gerados sinteticamente de c√≥digo aberto.

| Modelo   | Tamanho do Lote | Comprimento do Contexto | Tamanho Oculto | Tamanho Oculto FFN | Cabe√ßas de Aten√ß√£o | Cabe√ßas Chave-Valor | Camadas | Normaliza√ß√£o | Ativa√ß√£o | Tamanho do Vocabul√°rio |
|----------|------------------|-------------------------|----------------|--------------------|--------------------|----------------------|---------|--------------|----------|------------------------|
| 3B       | 2048             | 2048                   | 2560           | 10240              | 32                 | 32 (MHA)            | 32      | RMSNorm      | swiglu   | 49152                  |
| 8B       | 1024             | 4096                   | 4096           | 14336              | 32                 | 8 (GQA)             | 36      | RMSNorm      | swiglu   | 49152                  |
| 20B      | 576              | 8192                   | 6144           | 24576              | 48                 | 1 (MQA)             | 52      | LayerNorm    | gelu     | 49152                  |
| 34B      | 532              | 8192                   | 6144           | 24576              | 48                 | 1 (MQA)             | 88      | LayerNorm    | gelu     | 49152                  |

---
## 3. Dados

O treinamento dos modelos Granite √© projetado para balancear custo, desempenho e sustentabilidade:

1. **Rastreamento e Filtragem de Dados**
   - Os dados de pr√©-treinamento de c√≥digo foram obtidos a partir de uma combina√ß√£o de conjuntos de dados dispon√≠veis publicamente.
   - Os dados brutos foram filtrados para reter uma lista de 116 linguagens de programa√ß√£o, selecionadas dentre mais de 300 linguagens.

2. **Conjuntos de Dados de Linguagem Natural**
   - **Documentos da Web**: StackExchange, CommonCrawl.  
   - **Texto Matem√°tico da Web**: OpenWebMath, StackMathQA.  
   - **Texto Acad√™mico**: ArXiv, Wikipedia.  
   - **Conjuntos de Dados para Ajuste por Instru√ß√£o**: FLAN, HelpSteer.  

3. **T√©cnicas de Sustentabilidade**:
   - Uso de datacenters energeticamente eficientes.
   - Deduplica√ß√£o agressiva para otimizar os dados de treinamento.

---
## 4. Treinamento

Os modelos Granite Code s√£o treinados com 3,5 a 4,5 trilh√µes de tokens de dados de c√≥digo.

**Fase 1 (treinamento somente com c√≥digo):**
- Modelos de 3B e 8B s√£o treinados com 4 trilh√µes de tokens de dados de c√≥digo, abrangendo 116 idiomas.
- O modelo de 20B par√¢metros √© treinado com 3 trilh√µes de tokens de c√≥digo.
- O modelo de 34B √© treinado com 1,4 trilh√µes de tokens ap√≥s a amplia√ß√£o da profundidade, realizada no ponto de verifica√ß√£o de 1,6 trilh√µes do modelo de 20B.

**Fase 2 (treinamento com c√≥digo + linguagem):**
- Dados adicionais de alta qualidade e publicamente dispon√≠veis de diversos dom√≠nios, incluindo documentos t√©cnicos, matem√°ticos e da web, s√£o inclu√≠dos para melhorar ainda mais o desempenho do modelo em habilidades de racioc√≠nio e resolu√ß√£o de problemas, essenciais para a gera√ß√£o de c√≥digo.
- Todos os modelos s√£o treinados com 500B tokens (80% de dados de c√≥digo e 20% de dados de linguagem).

---
## 5. Downloads

| Modelo               | Caminho Hugging Face                                         |
|----------------------|-------------------------------------------------------------|
| Granite-3B-Instruct  | [ü§ó HuggingFace](https://huggingface.co/ibm-granite/granite-3.1-3b-a800m-instruct) |
| Granite-3B-Base      | [ü§ó HuggingFace](https://huggingface.co/ibm-granite/granite-3.1-3b-a800m-base) |
| Granite-8B-Instruct  | [ü§ó HuggingFace](https://huggingface.co/ibm-granite/granite-3.1-8b-instruct) |
| Granite-8B-Base      | [ü§ó HuggingFace](http://huggingface.co/ibm-granite/granite-3.1-8b-base)

Veja outros modelos Granite: https://huggingface.co/ibm-granite

---
## 6. Benchmarks

Para resultados detalhados, consulte: [IBM Granite Code Model GitHub](https://github.com/ibm-granite/granite-code-models/blob/main/README.md#evaluation-results).

---
## 7. Requisitos de Sistema

| Vers√£o do Modelo | VRAM (GPU)      | RAM (CPU)     | Armazenamento |
|------------------|-----------------|---------------|---------------|
| Granite **3B**            | 4GB+           | 8GB+          | 5GB           |
| Granite **8B**              | 12GB+          | 16GB+         | 10GB          |
| Granite **20B**             | 24GB+          | 64GB+         | 30GB          |
| Granite **34B**             | 80GB+          | 256GB+        | 120GB         |

---
## 8. Fontes

- [Artigo Granite Models](https://arxiv.org/abs/2405.04324)
- [Artigo Medium](https://ritvik19.medium.com/paper-explained-144-granite-code-models-e1a92678739b)
- [IBM Granite Oficial](https://www.ibm.com/granite)
- [GitHub Granite](https://github.com/ibm-granite/granite-code-models)
- [Blog Red Hat - Granite Models](https://www.redhat.com/en/blog/ibms-granite-foundation-model-detailed-look-its-training-data)
- [Wikipedia - IBM Granite](https://en.wikipedia.org/wiki/IBM_Granite)
- [Documenta√ß√£o Oficial](https://www.ibm.com/granite/docs/)
