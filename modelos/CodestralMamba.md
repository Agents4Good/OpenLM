## Codestral Mamba 7B

Modelo de linguagem da sÃ©rie Mamba2, desenvolvido para tarefas de geraÃ§Ã£o de cÃ³digo. Especializado em criar soluÃ§Ãµes de programaÃ§Ã£o de maneira eficiente. Oferece uma abordagem otimizada para a criaÃ§Ã£o de cÃ³digo, sendo Ãºtil para vÃ¡rias linguagens e tipos de desenvolvimento.

- **Desenvolvido por:** Mistral AI
- **Arquitetura:**  Baseada em Mamba.

---
## ðŸ” Arquitetura

Utiliza arquitetura de Mamba ao invÃ©s de tranformers, oferencendo a vantagem de inferÃªncia em tempo linear e a capacidade (teÃ³rica) de modelar sequÃªncias de comprimento infinito. Isso permite que os usuÃ¡rios interajam extensivamente com o modelo, recebendo respostas rÃ¡pidas, independentemente do tamanho da entrada. Especialmente relevante para casos de uso voltados Ã  anÃ¡lise e geraÃ§Ã£o de cÃ³digos de programaÃ§Ã£o.


| **Recurso**                | **Transformer**                           | **Mamba2**                                |
|----------------------------|------------------------------------------|-------------------------------------------|
| **Tipo de Arquitetura**     | Baseada em AtenÃ§Ã£o                       | Modelo de EspaÃ§o de Estado (SSM)          |
| **Mecanismo Central**       | Foca igualmente em todas as partes da entrada | Foca apenas nas partes mais importantes |
| **EficiÃªncia**              | Mais lento Ã  medida que o tamanho da entrada aumenta | Mais rÃ¡pido, mesmo com entradas grandes  |
| **Gerenciamento de Contexto** | Limitado pela quantidade de entrada que pode processar | Gerenciamento de contexto ilimitado      |
| **InovaÃ§Ãµes Principais**    | Divide a entrada em partes menores para entendÃª-la | Processa seletivamente a entrada, otimizando para velocidade |
| **Velocidade de Treinamento e InferÃªncia** | Mais lenta devido ao seu design       | Mais rÃ¡pida, otimizada para respostas rÃ¡pidas |


---
## ðŸ§ª Desempenho em Benchmarks

## **Benchmarks de cÃ³digo**
![Benchmark](./misc/codestral-mamba-benchmarks.png)
Fonte: [https://mistral.ai/news/codestral-mamba/](https://mistral.ai/news/codestral-mamba/)




---
## ðŸš€ Como Usar 
> Pelo mistral inference (recomendado).

### **InstalaÃ§Ã£o**
```bash
pip install mistral_inference>=1 mamba-ssm causal-conv1d
```

### **Download**
```python
from huggingface_hub import snapshot_download
from pathlib import Path

mistral_models_path = Path.home().joinpath('mistral_models', 'Mamba-Codestral-7B-v0.1')
mistral_models_path.mkdir(parents=True, exist_ok=True)

snapshot_download(repo_id="mistralai/Mamba-Codestral-7B-v0.1", allow_patterns=["params.json", "consolidated.safetensors", "tokenizer.model.v3"], local_dir=mistral_models_path)
```

### **Demo**
```bash
mistral-chat $HOME/mistral_models/Mamba-Codestral-7B-v0.1 --instruct  --max_tokens 256
```

### **Finetuning**
[mistral-finetune](https://github.com/mistralai/mistral-finetune).

---
## ðŸ“œ Fontes
https://huggingface.co/mistralai/Mamba-Codestral-7B-v0.1

https://mistral.ai/news/codestral-mamba/

https://hatchworks.com/blog/gen-ai/codestral-mamba-guide/
