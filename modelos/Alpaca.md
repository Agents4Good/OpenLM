# Modelo Alpaca

> O **Alpaca** √© um modelo de linguagem desenvolvido pelo Centro de Pesquisa em Fundamentos de Modelos (CRFM) da Universidade de Stanford. Trata-se de uma vers√£o ajustada do modelo LLaMA 7B da Meta, treinada para seguir instru√ß√µes de maneira semelhante ao modelo text-davinci-003 da OpenAI.

---

## üîç Caracter√≠sticas Principais

- **Criador:** Centro de Pesquisa em Fundamentos de Modelos (CRFM) da Universidade de Stanford
- **Arquitetura:** Baseada no modelo LLaMA 7B da Meta
- **Treinamento:** Ajustado utilizando um conjunto de 52.000 pares de instru√ß√£o-resposta gerados pelo modelo text-davinci-003 da OpenAI
- **Capacidades:**
  - Segue instru√ß√µes de forma eficaz
  - Gera respostas coerentes e informativas
  - Aplic√°vel em tarefas como resumo, tradu√ß√£o e perguntas e respostas

---

## üß™ Desempenho em Benchmarks

O desempenho do Alpaca foi avaliado utilizando o AlpacaEval, uma ferramenta de avalia√ß√£o autom√°tica baseada em LLMs. Abaixo est√° uma compara√ß√£o de desempenho entre diferentes modelos:
> Fonte: https://tatsu-lab.github.io/alpaca_eval/<br>
> GPT-4 foi usado como refer√™ncia nos testes.


| **Modelo**         | **Score GPT-4 (%)**  | **Desempenho Geral** |
|---------------------|---------------------|-----------------------|
| Alpaca 7B          | 90.2                 | Competitivo em tarefas de seguimento de instru√ß√µes |
| LLaMA 7B           | 85.6                 | Base do Alpaca, sem ajustes para instru√ß√µes        |
| text-davinci-003   | 95.8                 | Modelo propriet√°rio da OpenAI                      |

---

## üìö Pesquisas Relevantes

Confira o Blog da Universidade de Stanford: https://crfm.stanford.edu/blog.html

---

## üì• Vers√µes

| **Vers√£o** | **Par√¢metros** |
|-------------|----------------|
| Alpaca 7B   | 7 bilh√µes     |

---

## üíª Requisitos Recomendados

| **Vers√£o** | **Mem√≥ria RAM** | **Mem√≥ria da GPU** | **Armazenamento** |
|-------------|-----------------|----------------------|-------------------|
| Alpaca 7B   | 16 GB          | 12 GB               | 2 GB              |

---

## ‚úÖ Pr√≥s

1. **Acessibilidade:** Modelo de c√≥digo aberto, permitindo customiza√ß√µes e integra√ß√µes diversas.
2. **Efici√™ncia:** Desempenho competitivo em tarefas de seguimento de instru√ß√µes.
3. **Custo-efetivo:** Treinamento realizado com um conjunto de dados gerado de forma econ√¥mica.

---

## ‚ùå Contras

1. **Depend√™ncia de Dados Sint√©ticos:** O conjunto de dados de treinamento foi gerado por outro modelo de linguagem, o que pode introduzir vieses.
2. **Limita√ß√µes de Escalabilidade:** Como √© baseado no LLaMA 7B, pode n√£o alcan√ßar o desempenho de modelos maiores em tarefas complexas.
3. **Necessidade de Recursos Computacionais:** Embora menor que alguns modelos, ainda requer hardware significativo para treinamento e infer√™ncia.

---

## üöÄ Como Usar o Alpaca

### 1. **Instala√ß√£o**

Certifique-se de ter o Python instalado e clone o reposit√≥rio oficial:

```bash
git clone https://github.com/tatsu-lab/stanford_alpaca.git
cd stanford_alpaca
pip install -r requirements.txt
```
> Se ocorrer erro ao usar pip, tente pip3

### 2. **Carregar o Modelo**

Utilize a biblioteca `transformers` para carregar o modelo:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("WeOpenML/Alpaca-7B-v1")
model = AutoModelForCausalLM.from_pretrained("WeOpenML/Alpaca-7B-v1")
```

### 3. **Gera√ß√£o de Texto**

Gere texto com base em um prompt:

```python
input_text = "Explique a teoria da relatividade de forma simples."
inputs = tokenizer(input_text, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200, do_sample=True)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)
```

### 4. **Ajuste Fino (Fine-Tuning)**

Tutorial de ajuste de um Modelo Alpaca: https://www.stochasticbard.com/blog/alpaca-model-communicator/

---

## üìú Fontes

- [An√∫ncio Oficial do Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)
- [Reposit√≥rio no GitHub](https://github.com/tatsu-lab/stanford_alpaca)
- [Compara√ß√£o entre LLaMA e Alpaca](https://medium.com/@saluem/llama-vs-alpaca-ai-similarities-differences-c793f870aefd)
- [AlpacaEval Leaderboard](https://tatsu-lab.github.io/alpaca_eval/)
- [Modelo Alpaca no Hugging Face](https://huggingface.co/WeOpenML/Alpaca-7B-v1)
- [Como treinar uma Alpaca](https://radekosmulski.com/how-do-you-train-an-alpaca/)
