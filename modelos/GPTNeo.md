# Modelo GPT Neo

> O **GPT Neo** √© um modelo de linguagem baseado em transformadores, desenvolvido pela EleutherAI, projetado para gera√ß√£o de texto coerente e natural.
> Ele foi introduzido como uma alternativa de c√≥digo aberto aos modelos GPT da OpenAI.

---
## üîç Caracter√≠sticas Principais

- **Criador:** EleutherAI
- **Arquitetura:** Baseada em Transformers, utilizando camadas de decodificadores (decoders) com aten√ß√£o unidirecional.
- **Treinamento:** Pr√©-treinado em grandes corpora√ß√µes de texto, como The Pile, que inclui diversas fontes de dados.
- **Capacidades:**
  - Gera√ß√£o de texto coerente e criativo;
  - Completar frases e par√°grafos;
  - Suporte para tarefas como tradu√ß√£o, resumo e perguntas e respostas.

---
## üß™ Desempenho em Benchmarks

O GPT Neo √© avaliado principalmente em tarefas de gera√ß√£o de texto e NLP.

| **Benchmark**         | **Tarefa**                    | **M√©trica**       | **Resultado** | **Fonte**                                        |
|------------------------|-------------------------------|-------------------|---------------|------------------------------------------------|
| EleutherAI Evaluation | Gera√ß√£o de texto criativo     | Coer√™ncia         | Alta          | [Documenta√ß√£o GPT Neo](https://huggingface.co/docs/transformers/en/model_doc/gpt_neo) |
| The Pile Perplexity   | Qualidade do texto gerado     | Perplexidade      | Baixa         | [The Pile Dataset](https://arxiv.org/abs/2101.00027)            |

---
## üìö Pesquisas Relevantes

- Confira diversos artigos feitos pela **EleutherAI**: https://www.eleuther.ai/research

---
## ‚úÖ Pr√≥s
1. Acessibilidade: C√≥digo aberto, permitindo customiza√ß√µes e integra√ß√µes diversas;
2. Escalabilidade: Dispon√≠vel em tamanhos variados, adequados para diferentes aplica√ß√µes;
3. Efici√™ncia para tarefas de gera√ß√£o de texto: Altamente eficaz em criar textos criativos e coerentes.

---
## ‚ùå Contras
1. Requer hardware robusto: Modelos maiores exigem GPUs ou TPUs para treinamento e infer√™ncia;
2. Menor desempenho em compara√ß√£o com GPT propriet√°rios: Resultados ligeiramente inferiores aos modelos mais avan√ßados da OpenAI;
3. Limita√ß√£o de contexto: Restri√ß√µes no comprimento de entrada devido ao design da arquitetura.

---
## üöÄ Como Usar o GPT Neo

### 1. **Instala√ß√£o**
Certifique-se de ter o Python instalado e instale a biblioteca `transformers`:
```bash
pip install transformers
```

### 2. **Carregar o Modelo e o Tokenizer**
```python
from transformers import GPTNeoForCausalLM, AutoTokenizer

# Carregar tokenizer e modelo pr√©-treinado
modelo = "EleutherAI/gpt-neo-125M"
tokenizer = AutoTokenizer.from_pretrained(modelo)
model = GPTNeoForCausalLM.from_pretrained(modelo)
```

### 3. **Gera√ß√£o de Texto**
```python
# Entrada de exemplo
texto_entrada = "A intelig√™ncia artificial est√° revolucionando"

# Tokenizar entrada
inputs = tokenizer(texto_entrada, return_tensors="pt")

# Gerar texto
outputs = model.generate(inputs.input_ids, max_length=50, do_sample=True)

# Decodificar sa√≠da
teste_saida = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(teste_saida)
```

### 4. **Ajuste Fino (Fine-Tuning)**
O GPT Neo pode ser ajustado para tarefas espec√≠ficas utilizando datasets customizados. O ajuste fino √© realizado com bibliotecas como `transformers` e `datasets`:
```python
from transformers import Trainer, TrainingArguments

# Configura√ß√µes do treinamento
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=2,
    num_train_epochs=3,
    logging_dir="./logs",
    save_steps=10_000,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=seu_dataset_treino,
    eval_dataset=seu_dataset_validacao
)

trainer.train()
```

---
## üìú Fontes

- [Documenta√ß√£o do GPT Neo](https://huggingface.co/docs/transformers/en/model_doc/gpt_neo)
- [The Pile Dataset](https://arxiv.org/abs/2101.00027)
- [Reposit√≥rio da EleutherAI](https://github.com/EleutherAI)

