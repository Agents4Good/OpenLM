# Modelo GPT Neo

> O **GPT Neo** √© um modelo de linguagem baseado em transformadores, desenvolvido pela EleutherAI, projetado para gera√ß√£o de texto coerente e natural.
> Ele foi introduzido como uma alternativa de c√≥digo aberto aos modelos GPT da OpenAI.

---
## üîç Caracter√≠sticas Principais

- **Criador:** EleutherAI
- **Arquitetura:** Baseada em Transformers, utilizando camadas de decodificadores (decoders) com aten√ß√£o unidirecional.
- **Treinamento:** Pr√©-treinado em grandes corpora√ß√µes de texto, como The Pile, que inclui diversas fontes de dados.
- **Capacidades:**
  - Gera√ß√£o de texto coerente e criativo;
  - Completar frases e par√°grafos;
  - Suporte para tarefas como tradu√ß√£o, resumo e perguntas e respostas.

---
## üß™ Desempenho em Benchmarks

O GPT Neo √© avaliado principalmente em tarefas de gera√ß√£o de texto e NLP.
> Fonte: [Artigo - Medium](https://medium.com/georgian-impact-blog/gpt-neo-vs-gpt-3-are-commercialized-nlp-models-really-that-much-better-f4c73ffce10b)

| **Modelo**          | **0-shot (%)** | **1-shot (%)** | **4-shot (%)** | **8-shot (%)** | **Custo 0-shot** | **Custo 8-shot** | **Acur√°cia Geral (%)** |
|----------------------|----------------|----------------|----------------|----------------|------------------|------------------|-------------------------|
| GPT-Neo 1.3B         | 56.9          | 84.4           | 83.5           | 78.1           | $0               | $0               | 84.4                   |
| GPT-Neo 2.7B         | 6.97          | 73.1           | 84.4           | 68.0           | $0               | $0               | 84.4                   |
| GPT-3 ada (2.7B)     | 47.6          | 60.8           | 72.6           | 62.5           | $0.04/run        | $0.44/run        | 88.4                   |
| GPT-3 curie (13B)    | 45.1          | 83.4           | 89.3           | 86.1           | $0.33/run        | $3.35/run        | 95.1                   |
| GPT-3 davinci (175B) | 36.7          | 85.8           | 94.3           | 95.1           | $3.26/run        | $33.43/run       | 95.1                   |
| SMART-RoBERTa Large  | -             | -              | -              | -              | -                | -                | 97.5                   |

---
## üìö Pesquisas Relevantes

- Confira diversos artigos feitos pela **EleutherAI**: https://www.eleuther.ai/research

---
## üì• Vers√µes

| **Vers√£o**         | **Camadas** | **Cabe√ßas de Aten√ß√£o** | **Dimens√µes Ocultas** | **Dimens√µes Feed-Forward** | **Par√¢metros** |
|---------------------|-------------|------------------------|------------------------|----------------------------|----------------|
| GPT Neo 125M       | 12          | 12                     | 768                    | 3072                       | 125 milh√µes    |
| GPT Neo 1.3B       | 24          | 16                     | 2048                   | 8192                       | 1,3 bilh√µes    |
| GPT Neo 2.7B       | 32          | 20                     | 2560                   | 10240                      | 2,7 bilh√µes    |

---
## üíª‚Äã Requisitos Recomendados

| **Vers√£o**         | **Mem√≥ria RAM** | **Mem√≥ria da GPU** | **Armazenamento** |
|---------------------|-----------------|--------------------|-------------------|
| GPT Neo 125M       | 8 GB            | 4 GB               | 500 MB            |
| GPT Neo 1.3B       | 16 GB           | 12 GB              | 2 GB              |
| GPT Neo 2.7B       | 32 GB           | 24 GB              | 5 GB              |

---
## ‚úÖ Pr√≥s
1. **Acessibilidade**: C√≥digo aberto, permitindo customiza√ß√µes e integra√ß√µes diversas;
2. **Escalabilidade**: Dispon√≠vel em tamanhos variados, adequados para diferentes aplica√ß√µes;
3. Efici√™ncia para tarefas de **gera√ß√£o de texto**: Altamente eficaz em criar textos criativos e coerentes.

---
## ‚ùå Contras
1. Requer **hardware robusto**: Modelos maiores exigem GPUs ou TPUs para treinamento e infer√™ncia;
2. **Menor desempenho** em compara√ß√£o com GPT propriet√°rios: Resultados ligeiramente inferiores aos modelos mais avan√ßados da OpenAI;
3. **Limita√ß√£o de contexto**: Restri√ß√µes no comprimento de entrada devido ao design da arquitetura.

---
## üöÄ Como Usar o GPT Neo

### 1. **Instala√ß√£o**
Certifique-se de ter o Python instalado e instale a biblioteca `transformers`:
```bash
pip3 install transformers

git clone https://github.com/EleutherAI/GPTNeo
cd GPTNeo
pip3 install -r requirements.txt
```

### 2. **Configurar com Par√¢metros**
```python
from transformers import GPTNeoConfig, GPTNeoModel

# Initializing a GPTNeo EleutherAI/gpt-neo-1.3B style configuration
configuration = GPTNeoConfig()

# Initializing a model (with random weights) from the EleutherAI/gpt-neo-1.3B style configuration
model = GPTNeoModel(configuration)

# Accessing the model configuration
configuration = model.config
```

### 3. **Uso de Tokens**
```python
from transformers import AutoTokenizer, GPTNeoModel
import torch

tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = GPTNeoModel.from_pretrained("EleutherAI/gpt-neo-1.3B")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
```

### 4. **Gera√ß√£o de Texto**
```python
from transformers import GPTNeoForCausalLM, GPT2Tokenizer

model = GPTNeoForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")
tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")

prompt = (
    "In a shocking finding, scientists discovered a herd of unicorns living in a remote, "
    "previously unexplored valley, in the Andes Mountains. Even more surprising to the "
    "researchers was the fact that the unicorns spoke perfect English."
)

input_ids = tokenizer(prompt, return_tensors="pt").input_ids

gen_tokens = model.generate(
    input_ids,
    do_sample=True,
    temperature=0.9,
    max_length=100,
)
gen_text = tokenizer.batch_decode(gen_tokens)[0]
```

### 5. **Ajuste Fino (Fine-Tuning)**
O GPT Neo pode ser ajustado para tarefas espec√≠ficas utilizando datasets customizados. O ajuste fino √© realizado com bibliotecas como `transformers` e `datasets`:
```python
from transformers import Trainer, TrainingArguments

# Configura√ß√µes do treinamento
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=2,
    num_train_epochs=3,
    logging_dir="./logs",
    save_steps=10_000,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=seu_dataset_treino,
    eval_dataset=seu_dataset_validacao
)

trainer.train()
```

---
## üìú Fontes

- [Documenta√ß√£o do GPT Neo](https://huggingface.co/docs/transformers/en/model_doc/gpt_neo)
- [The Pile Dataset](https://arxiv.org/abs/2101.00027)
- [Reposit√≥rio da EleutherAI](https://github.com/EleutherAI)

