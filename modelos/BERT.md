# Modelo BERT

> O **BERT (Bidirectional Encoder Representations from Transformers)** √© um modelo de linguagem natural criado pelo Google AI Research, projetado para compreender o contexto de palavras em uma frase de forma bidirecional. Ele foi introduzido em 2018 no artigo ["BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"](https://arxiv.org/abs/1810.04805).

---
## üîç Caracter√≠sticas Principais

- **Criador:** Google AI Research
- **Arquitetura:** Baseada em Transformers, com camadas empilhadas de codificadores (encoders) que processam as entradas bidirecionalmente.
- **Treinamento:** Pr√©-treinado em grandes corpora de texto, como Wikipedia e BookCorpus.
- **Objetivos de Treinamento:**
  - *Masked Language Model (MLM):* Predi√ß√£o de palavras mascaradas no texto.
  - *Next Sentence Prediction (NSP):* Predi√ß√£o se uma frase segue outra em um par.

---
## üß™ Desempenho em Benchmarks

O BERT foi avaliado em diversos benchmarks e mostrou resultados de ponta em v√°rias tarefas de Processamento de Linguagem Natural (NLP).

| **Benchmark**         | **Tarefa**                    | **M√©trica**       | **Resultado** | **Fonte**                                                                 |
|------------------------|-------------------------------|-------------------|---------------|---------------------------------------------------------------------------|
| SQuAD 1.1             | Respostas a perguntas         | Exatid√£o          | 93.2%         | [Artigo Original](https://arxiv.org/abs/1810.04805)                        |
| GLUE                  | Classifica√ß√£o de senten√ßas    | Pontua√ß√£o m√©dia   | 80.5          | [GLUE Leaderboard](https://gluebenchmark.com/leaderboard)                |
| CoLA                  | Aceitabilidade lingu√≠stica    | Matthews Corr.    | 60.5          | [Resultados em CoLA](https://nyu-mll.github.io/CoLA/)                    |

---

## üìö Pesquisas Relevantes

- **Fine-Tuning de Tarefas Espec√≠ficas:** Pesquisas demonstram que o BERT pode ser ajustado para v√°rias tarefas, incluindo classifica√ß√£o de texto, tradu√ß√£o e an√°lise de sentimentos.
- **Impacto em Modelos Posteriores:** O BERT influenciou a cria√ß√£o de modelos avan√ßados como RoBERTa, DistilBERT e ALBERT.

---
## üöÄ Como Usar o BERT

### 1. **Instala√ß√£o**
Certifique-se de ter o Python instalado e instale a biblioteca `transformers`:
```bash
pip install transformers
```

### 2. **Carregar o Modelo e o Tokenizer**
```python
from transformers import BertTokenizer, BertModel

# Carregar tokenizer e modelo pr√©-treinado
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")
```

### 3. **Tokeniza√ß√£o**
```python
# Entrada de exemplo
texto = "O BERT √© incr√≠vel para NLP!"

# Tokenizar entrada
tokens = tokenizer(texto, return_tensors="pt")
print(tokens)
```

### 4. **Infer√™ncia**
```python
# Passar tokens pelo modelo
output = model(**tokens)

# Representa√ß√£o dos tokens
print(output.last_hidden_state)
```

---

## üìú Artigos

- [Artigo Original do BERT](https://arxiv.org/abs/1810.04805)
- [Documenta√ß√£o do Hugging Face](https://huggingface.co/docs/transformers/en/model_doc/bert)
- [Resultados em Benchmarks](https://gluebenchmark.com/leaderboard)
