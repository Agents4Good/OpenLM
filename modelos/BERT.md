# Modelo BERT

> O **BERT (Bidirectional Encoder Representations from Transformers)** √© um modelo de linguagem natural criado pelo Google AI Research, projetado para compreender o contexto de palavras em uma frase de forma bidirecional. Ele foi introduzido em 2018 no artigo ["BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"](https://arxiv.org/abs/1810.04805).

---
## üîç Caracter√≠sticas Principais

- **Criador:** Google AI Research
- **Arquitetura:** Baseada em Transformers, com camadas de codificadores (encoders) que processam as entradas bidirecionalmente.
- **Treinamento:** Pr√©-treinado em grandes corpora√ß√µes de texto, como Wikipedia e BookCorpus.
- **Objetivos de Treinamento:**
  - *Masked Language Model (MLM):* Predi√ß√£o de palavras mascaradas no texto.
  - *Next Sentence Prediction (NSP):* Predi√ß√£o de frases.

---
## üß™ Desempenho em Benchmarks

O BERT foi avaliado em diversos benchmarks e mostrou resultados de ponta em v√°rias tarefas de Processamento de Linguagem Natural (NLP).
> Fontes: [M√©dium](https://medium.com/@kefactor/benchmarking-bert-based-models-for-text-classification-7182db4df89a) e [Artigo Oficial](https://arxiv.org/abs/1810.04805)

| **Benchmark**         | **Modelo Avaliado**  | **Tarefa**                    | **M√©trica**       | **Resultado** |
|-----------------------|---------------------|--------------------------------|-------------------|---------------|
| SQuAD 1.1             | BERT large          | Respostas a perguntas         | Exatid√£o          | 93.2%          | 
| GLUE                  | BERT base           | Classifica√ß√£o de senten√ßas    | Pontua√ß√£o m√©dia   | 80.5           | 
| CoLA                  | BERT base           | Aceitabilidade lingu√≠stica    | Matthews Corr.    | 60.5           |
| MNLI                  | BERT large          | Infer√™ncia textual            | Exatid√£o          | 84.6%          | 
| SST-2                 | BERT base           | An√°lise de sentimentos        | Exatid√£o          | 94.9%          | 
| RACE                  | BERT large          | Compreens√£o de leitura        | Taxa de acerto    | 65.0%          | 
| QNLI                  | BERT base           | Quest√µes naturais             | Exatid√£o          | 91.2%          |

---
## üìö Pesquisas Relevantes

- **Fine-Tuning de Tarefas Espec√≠ficas:** Pesquisas demonstram que o BERT pode ser ajustado para v√°rias tarefas, incluindo classifica√ß√£o de texto, tradu√ß√£o e an√°lise de sentimentos.
- **Impacto em Modelos Posteriores:** O BERT influenciou a cria√ß√£o de modelos avan√ßados como RoBERTa, DistilBERT e ALBERT.

---
## üì• Vers√µes

| Vers√£o     | Camadas | Cabe√ßas de Aten√ß√£o | Dimens√µes Ocultas | Dimens√µes Feed-Forward | Par√¢metros    |
|------------|---------|--------------------|-------------------|------------------------|---------------|
| **BERT base** | 12      | 12                 | 768               | 3072                   | 110 milh√µes  |
| **BERT large**| 24      | 16                 | 1024              | 4096                   | 340 milh√µes  |

---
## üíª Requisitos Recomendados

| Vers√£o         | Mem√≥ria RAM          | Processador                           |  Observa√ß√µes                            |
|----------------|----------------------|---------------------------------------|-----------------------------------------|
| **BERT base**  | 16 GB                | CPU com 4 n√∫cleos (ex. i7 ou Ryzen 5) | Ideal para tarefas moderadas de NLP     |
| **BERT large** | 32 GB                | CPU com 8 n√∫cleos (ex. i9 ou Ryzen 7) | Necessita de maior poder computacional  |

---
## ‚úÖ Pr√≥s
1. **Compreens√£o do Contexto**: Altamente eficaz em entender o contexto completo de uma frase;
2. **Processamento Bidirecional**: O BERT processa o texto de maneira bidirecional (ambos os lados do texto), proporcionando maior precis√£o nas respostas;
3. **Ideal para classifica√ß√£o de texto**: O BERT foi projetado para compreens√£o e classifica√ß√£o de texto.

---
## ‚ùå Contras
1. **Pesado**: O BERT, especialmente nas vers√µes maiores como a Large, √© pesado e requer grande poder computacional;
2. **Tempo de Infer√™ncia alto**: Devido √† sua arquitetura bidirecional, o tempo de processamento √© demorado;
3. **Limite de Caracteres em uma frase**: O BERT tem um limite de tamanho de sequ√™ncia (geralmente 512 tokens);
4. **N√£o √© ideal para gera√ß√£o de texto**: O BERT foi projetado para compreens√£o e classifica√ß√£o de texto, n√£o para gera√ß√£o.

---
## üöÄ Como Usar o BERT

### 1. **Instala√ß√£o**
Certifique-se de ter o Python instalado e instale a biblioteca `transformers`:
```bash
pip install transformers
```

### 2. **Carregar o Modelo e o Tokenizer**
```python
from transformers import AutoTokenizer, BertModel
import torch

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
model = BertModel.from_pretrained("google-bert/bert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
```

### 3. **Tokeniza√ß√£o**
```python
from transformers import BertConfig, BertModel

# Initializing a BERT google-bert/bert-base-uncased style configuration
configuration = BertConfig()

# Initializing a model (with random weights) from the google-bert/bert-base-uncased style configuration
model = BertModel(configuration)

# Accessing the model configuration
configuration = model.config
```

### 4. **Multiplas Escolhas**
```python
from transformers import AutoTokenizer, TFBertForMultipleChoice
import tensorflow as tf

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
model = TFBertForMultipleChoice.from_pretrained("google-bert/bert-base-uncased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits
```

### 5. **Classifica√ß√£o de Inputs**
> O BERT √© frequentemente usado como modelo em tarefas de classifica√ß√£o, como an√°lise de sentimentos.

```python
import torch
from transformers import AutoTokenizer, BertForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("textattack/bert-base-uncased-yelp-polarity")
model = BertForSequenceClassification.from_pretrained("textattack/bert-base-uncased-yelp-polarity")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]

# To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
num_labels = len(model.config.id2label)
model = BertForSequenceClassification.from_pretrained("textattack/bert-base-uncased-yelp-polarity", num_labels=num_labels)

labels = torch.tensor([1])
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)
```

### 6. Perguntas e Respostas

```python
from transformers import AutoTokenizer, TFBertForQuestionAnswering
import tensorflow as tf

tokenizer = AutoTokenizer.from_pretrained("ydshieh/bert-base-cased-squad2")
model = TFBertForQuestionAnswering.from_pretrained("ydshieh/bert-base-cased-squad2")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="tf")
outputs = model(**inputs)

answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])
answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)
```

---

## üìú Fontes

- [Artigo Original do BERT](https://arxiv.org/abs/1810.04805)
- [Documenta√ß√£o do Hugging Face](https://huggingface.co/docs/transformers/en/model_doc/bert)
- [Benchmarking BERT-based Models](https://medium.com/@kefactor/benchmarking-bert-based-models-for-text-classification-7182db4df89a)
- [Wikip√©dia](https://en.wikipedia.org/wiki/BERT_(language_model))
- [BERT Model](https://snorkel.ai/large-language-models/bert-models/)
- [Artigo Medium](https://medium.com/@vipra_singh/llm-architectures-explained-bert-part-8-c60c1d9ebc82)
- [SQuAD Dataset](https://rajpurkar.github.io/SQuAD-explorer/)
- [CoLA Dataset](https://nyu-mll.github.io/CoLA/)
- [RACE Dataset](http://www.cs.cmu.edu/~glai1/data/race/)
- [MNLI Benchmark](https://cims.nyu.edu/~sbowman/multinli/)
- [SST-2 Dataset](https://nlp.stanford.edu/sentiment/treebank.html)
- [QNLI Benchmark](https://gluebenchmark.com/tasks)

