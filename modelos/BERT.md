# Modelo BERT

> O **BERT (Bidirectional Encoder Representations from Transformers)** √© um modelo de linguagem natural criado pelo Google AI Research, projetado para compreender o contexto de palavras em uma frase de forma bidirecional. Ele foi introduzido em 2018 no artigo ["BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"](https://arxiv.org/abs/1810.04805).

---
## üîç Caracter√≠sticas Principais

- **Criador:** Google AI Research
- **Arquitetura:** Baseada em Transformers, com camadas de codificadores (encoders) que processam as entradas bidirecionalmente.
- **Treinamento:** Pr√©-treinado em grandes corpora√ß√µes de texto, como Wikipedia e BookCorpus.
- **Objetivos de Treinamento:**
  - *Masked Language Model (MLM):* Predi√ß√£o de palavras mascaradas no texto.
  - *Next Sentence Prediction (NSP):* Predi√ß√£o de frases.

---
## üß™ Desempenho em Benchmarks

O BERT foi avaliado em diversos benchmarks e mostrou resultados de ponta em v√°rias tarefas de Processamento de Linguagem Natural (NLP).

| **Benchmark**         | **Tarefa**                    | **M√©trica**       | **Resultado** | **Fonte**                                                                 |
|------------------------|-------------------------------|-------------------|---------------|---------------------------------------------------------------------------|
| SQuAD 1.1             | Respostas a perguntas         | Exatid√£o          | 93.2%         | [Artigo Original](https://arxiv.org/abs/1810.04805)                        |
| GLUE                  | Classifica√ß√£o de senten√ßas    | Pontua√ß√£o m√©dia   | 80.5          | [GLUE Leaderboard](https://gluebenchmark.com/leaderboard)                |
| CoLA                  | Aceitabilidade lingu√≠stica    | Matthews Corr.    | 60.5          | [Resultados em CoLA](https://nyu-mll.github.io/CoLA/)                    |

---
## üìö Pesquisas Relevantes

- **Fine-Tuning de Tarefas Espec√≠ficas:** Pesquisas demonstram que o BERT pode ser ajustado para v√°rias tarefas, incluindo classifica√ß√£o de texto, tradu√ß√£o e an√°lise de sentimentos.
- **Impacto em Modelos Posteriores:** O BERT influenciou a cria√ß√£o de modelos avan√ßados como RoBERTa, DistilBERT e ALBERT.

---
## ‚úÖ Pr√≥s
1. Compreens√£o do Contexto: Altamente eficaz em entender o contexto completo de uma frase;
2. Processamento Bidirecional: O BERT processa o texto de maneira bidirecional (ambos os lados do texto), proporcionando maior precis√£o nas respostas;
3. Ideal para classifica√ß√£o de texto: O BERT foi projetado para compreens√£o e classifica√ß√£o de texto.

---
## ‚ùå Contras
1. Pesado: O BERT, especialmente nas vers√µes maiores como a Large, √© pesado e requer grande poder computacional;
2. Tempo de Infer√™ncia alto: Devido √† sua arquitetura bidirecional, o tempo de processamento √© demorado;
3. Limite de Caracteres em uma frase: O BERT tem um limite de tamanho de sequ√™ncia (geralmente 512 tokens);
4. N√£o √© ideal para gera√ß√£o de texto: O BERT foi projetado para compreens√£o e classifica√ß√£o de texto, n√£o para gera√ß√£o.

---
## üöÄ Como Usar o BERT

### 1. **Instala√ß√£o**
Certifique-se de ter o Python instalado e instale a biblioteca `transformers`:
```bash
pip install transformers
```

### 2. **Carregar o Modelo e o Tokenizer**
```python
from transformers import BertTokenizer, BertModel

# Carregar tokenizer e modelo pr√©-treinado
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")
```

### 3. **Tokeniza√ß√£o**
```python
# Entrada de exemplo
texto = "O BERT √© incr√≠vel para NLP!"

# Tokenizar entrada
tokens = tokenizer(texto, return_tensors="pt")
print(tokens)
```

### 4. **Infer√™ncia**
```python
# Passar tokens pelo modelo
output = model(**tokens)

# Representa√ß√£o dos tokens
print(output.last_hidden_state)
```

### 5. **Classifica√ß√£o de Inputs**
> O BERT √© frequentemente usado como modelo em tarefas de classifica√ß√£o, como an√°lise de sentimentos.

```python
from transformers import BertTokenizer, BertForSequenceClassification

# Carregar tokenizer e modelo pr√©-treinado para classifica√ß√£o
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# Texto de entrada
texto = "Este produto √© excelente!"

# Tokenizar entrada
inputs = tokenizer(texto, return_tensors="pt")

# Realizar previs√£o
outputs = model(**inputs)

# Logits (sa√≠das brutas)
logits = outputs.logits
print(logits)

# Predi√ß√£o final (0 ou 1, dependendo do mapeamento de r√≥tulos)
predicao = torch.argmax(logits, dim=1)
print(f"Sentimento: {'Positivo' if predicao == 1 else 'Negativo'}")
```

### 6. Perguntas e Respostas

```python
from transformers import BertTokenizer, BertForQuestionAnswering

# Carregar tokenizer e modelo para perguntas e respostas
tokenizer = BertTokenizer.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")
model = BertForQuestionAnswering.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")

# Contexto e pergunta
contexto = "A intelig√™ncia artificial tem muitos usos no mundo moderno, incluindo NLP."
pergunta = "Quais s√£o os usos da intelig√™ncia artificial?"

# Preparar entrada
inputs = tokenizer.encode_plus(pergunta, contexto, return_tensors="pt")

# Obter respostas do modelo
outputs = model(**inputs)
start_scores = outputs.start_logits
end_scores = outputs.end_logits

# Identificar tokens da resposta
start_index = torch.argmax(start_scores)
end_index = torch.argmax(end_scores)

# Decodificar resposta
resposta = tokenizer.decode(inputs.input_ids[0][start_index:end_index + 1])
print(f"Resposta: {resposta}")
```

---

## üìú Artigos

- [Artigo Original do BERT](https://arxiv.org/abs/1810.04805)
- [Documenta√ß√£o do Hugging Face](https://huggingface.co/docs/transformers/en/model_doc/bert)
- [Resultados em Benchmarks](https://gluebenchmark.com/leaderboard)
