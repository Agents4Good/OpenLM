<div align="center">
  <a href="https://huggingface.co/blog/falcon2-11b" target="_blank">
    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Falcon2%20LLM-ffc107?color=ffc107&logoColor=white" />
  </a>
</div>

<p align="center">
  <a href="https://arxiv.org/html/2407.14885v1"><b>üìù Paper - Falcon LLM</b></a>
</p>

---
## 1. Introdu√ß√£o

O **Falcon 2** √© uma nova gera√ß√£o de modelos de linguagem de c√≥digo aberto, oferecendo capacidades multil√≠ngues e multimodais. √â o √∫nico modelo de IA com capacidade de **vis√£o-para-linguagem (VLM)**, permitindo a interpreta√ß√£o de imagens e sua convers√£o para texto de maneira eficiente.

O **Falcon 2 11B** superou o **Llama 3 8B** da Meta e apresenta desempenho equivalente ao **Google Gemma 7B**, conforme verificado independentemente pelo **Hugging Face Leaderboard**. O pr√≥ximo passo da equipe √© a implementa√ß√£o de um **Mixture of Experts (MoE)** para ampliar ainda mais as capacidades do Falcon 2.

---
## 2. Arquitetura do Modelo

A arquitetura do **Falcon 2** foi projetada para ser altamente eficiente e escal√°vel, com avan√ßos significativos em compara√ß√£o √† sua vers√£o anterior.

Principais inova√ß√µes:
- **Multimodalidade**: Suporte nativo para tarefas que combinam vis√£o e linguagem.
- **Efici√™ncia Computacional**: Melhor uso de recursos de hardware e infer√™ncia mais r√°pida.
- **Escalabilidade**: Projetado para ser treinado e executado em larga escala com efici√™ncia.
- **Treinamento Otimizado**: Uso de dados diversificados para melhorar a capacidade de generaliza√ß√£o do modelo.

O Falcon 2 ser√° aprimorado no futuro com a adi√ß√£o do **Mixture of Experts (MoE)**, que permitir√° que diferentes partes do modelo sejam ativadas para tarefas espec√≠ficas, melhorando a efici√™ncia e o desempenho.

---
## 3. Modelos Dispon√≠veis

| Tamanho do Modelo | Descri√ß√£o |
|-------------------|-------------------------------------------------|
| Falcon 2 **11B**         | Modelo base para tarefas de linguagem natural. |
| Falcon 2 **11B VLM**     | Vers√£o multimodal com suporte a vis√£o-para-linguagem. |

---
## 4. Treinamento e Dados

O Falcon 2 foi treinado em um conjunto de dados diversificado e extensivo, cobrindo v√°rios idiomas e dom√≠nios para garantir melhor desempenho e generaliza√ß√£o.

Principais caracter√≠sticas do treinamento:
- **Baseado em um corpus multimodal**: Incluir imagens e textos melhora a capacidade de racioc√≠nio visual do modelo.
- **Uso de t√©cnicas avan√ßadas de otimiza√ß√£o**: Aprimoramento no processo de ajuste fino para tarefas espec√≠ficas.
- **Redu√ß√£o de vi√©s**: Maior diversidade de dados para tornar o modelo mais equitativo.

---
## 5. Benchmarks

Os testes de desempenho do Falcon 2 mostram supera√ß√£o do Llama 3 8B e desempenho equivalente ao Google Gemma 7B, conforme avalia√ß√µes independentes do Hugging Face Leaderboard.

- **Melhor desempenho em tarefas de linguagem natural em rela√ß√£o ao Llama 3 8B.**
- **Efici√™ncia competitiva com modelos da Google, como o Gemma 7B.**
- **Resultados promissores em benchmarks multimodais.**

Para mais detalhes, consulte os resultados completos no [Hugging Face Leaderboard](https://huggingface.co/blog/falcon2-11b).

---
## 6. Refer√™ncias

- [Artigo Falcon 2](https://arxiv.org/html/2407.14885v1)
- [Hugging Face Falcon 2](https://huggingface.co/blog/falcon2-11b)
- [Site Oficial](https://falconllm.tii.ae/falcon-models.html)

