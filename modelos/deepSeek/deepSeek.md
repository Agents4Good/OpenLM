<div align="center">
  <img src="https://github.com/Agents4Good/LMGuide/blob/main/imagens/logoDeepSeek.svg" width="60%" alt="DeepSeek LLM" />
</div>

<div align="center">
  <a href="https://huggingface.co/deepseek-ai" target="_blank">
    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" />
  </a>
</div>

<p align="center">
  <a href="https://arxiv.org/abs/2401.02954"><b>Paper Link</b>üëÅÔ∏è</a>
</p>

---
## 1. Introdu√ß√£o

DeepSeek-R1 √© o modelo de racioc√≠nio de primeira gera√ß√£o da DeepSeek, com desempenho compar√°vel ao OpenAI-o1. Ele est√° dispon√≠vel em v√°rias vers√µes, otimizadas para diferentes capacidades computacionais:

| Tamanho do Modelo | Descri√ß√£o                                                                 |
|-------------------|---------------------------------------------------------------------------|
| **1.5B**          | Vers√£o leve, otimizada para infer√™ncia r√°pida em dispositivos de borda.    |
| **7B**            | Modelo balanceado, adequado para tarefas gerais de racioc√≠nio.             |
| **8B**            | Maior precis√£o e melhor compreens√£o contextual.                           |
| **14B**           | Capacidades aprimoradas de racioc√≠nio e solu√ß√£o de problemas.              |
| **32B**           | An√°lise l√≥gica mais forte e sa√≠das detalhadas.                            |
| **70B**           | Vers√£o avan√ßada para aplica√ß√µes de IA de alto n√≠vel.                      |
| **671B**          | Modelo Mixture-of-Experts (MoE), ativando 37 bilh√µes de par√¢metros por token para desempenho de racioc√≠nio de √∫ltima gera√ß√£o. |

---
## 2. Downloads

> "R1" indica que esses modelos s√£o vers√µes distiladas de arquiteturas maiores, como Qwen e Llama, voltadas para um prop√≥sito espec√≠fico.<br>
> O processo de distilla√ß√£o reduz o tamanho e a complexidade do modelo enquanto tenta manter um desempenho pr√≥ximo ao do modelo maior.<br>
> "LLM" refere-se a modelos maiores e mais gerais.<br>
> "Base" refere-se a modelos para tarefas gerais de NLP.<br>
> "Chat" refere-se a modelos ajustados para tarefas conversacionais.

### Huggingface
|         Model         |                                Download                                    |                  
|:---------------------:|--------------------------------------------------------------------------: |
| DeepSeek-R1-Distill-Qwen-1.5B	| ü§ó [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |
| DeepSeek-R1-Distill-Qwen-7B   | ü§ó [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)     |
| DeepSeek-R1-Distill-Llama-8B	| ü§ó [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)    |
| DeepSeek-R1-Distill-Llama-70B	| ü§ó [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |
| DeepSeek LLM 7B Base  | ü§ó [HuggingFace](https://huggingface.co/deepseek-ai/deepseek-llm-7b-base)  |
| DeepSeek LLM 7B Chat  | ü§ó [HuggingFace](https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat)  |
| DeepSeek LLM 67B Base | ü§ó [HuggingFace](https://huggingface.co/deepseek-ai/deepseek-llm-67b-base) |
| DeepSeek LLM 67B Chat | ü§ó [HuggingFace](https://huggingface.co/deepseek-ai/deepseek-llm-67b-chat) |

---
## 3. Benchmarks

> Consulte aqui: https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/README.md#3-evaluation-results

---
## 4. Compara√ß√£o com outros Modelos

Ao contr√°rio dos modelos fechados, o DeepSeek-R1 oferece visibilidade do seu processo de racioc√≠nio passo a passo, permitindo aos usu√°rios rastrear, verificar e refinar as conclus√µes geradas pela IA.

Al√©m disso, por ser open-source sob a licen√ßa MIT, o DeepSeek-R1 oferece diversas vantagens:
- **Personaliza√ß√£o sem Restri√ß√µes**: Desenvolvedores podem ajustar e modificar o modelo para casos de uso espec√≠ficos, sem limita√ß√µes de licenciamento.
- **Melhorias pela Comunidade**: Pesquisadores e especialistas podem contribuir com aprimoramentos.
- **Efici√™ncia de Custos**: √â uma alternativa ideal para empresas que buscam integrar IA sem custos recorrentes.

---
## 5. Requisitos de Sistema

| Vers√£o do Modelo | VRAM (GPU)      | RAM (CPU)     | Armazenamento |
|------------------|-----------------|---------------|---------------|
| 1.5B            | 4GB+           | 8GB+          | 5GB           |
| 7B              | 12GB+          | 16GB+         | 10GB          |
| 8B              | 16GB+          | 32GB+         | 15GB          |
| 14B             | 24GB+          | 64GB+         | 30GB          |
| 32B             | 48GB+          | 128GB+        | 60GB          |
| 70B             | 80GB+          | 256GB+        | 120GB         |
| 671B (MoE)      | 4x A100 GPUs (320GB VRAM) | 512GB+ | 500GB+       |

---
## 6. Executando o DeepSeek-R1 Localmente com Ollama

O Ollama facilita a execu√ß√£o do DeepSeek-R1 localmente em seu sistema, sem a necessidade de APIs baseadas na nuvem. Os requisitos do sistema variam dependendo do tamanho do modelo escolhido.

#### Instalar o Ollama
Se ainda n√£o tiver o Ollama instalado, use o seguinte comando para configur√°-lo em sua m√°quina:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### Baixar e Configurar o DeepSeek-R1
Ap√≥s instalar o Ollama, √© poss√≠vel baixar o modelo DeepSeek-R1:

```bash
ollama pull deepseek-r1:7b
```

> A vers√£o "7b" refere-se ao modelo de 7 bilh√µes de par√¢metros. Substitua por "1.5b", "8b", "14b", "32b", "70b" ou at√© mesmo "671b".

#### Executar o DeepSeek-R1 Localmente
Para iniciar uma sess√£o interativa com o modelo, execute:

```bash
ollama run deepseek-r1:7b
```

Este comando permite enviar prompts e receber respostas em tempo real.
