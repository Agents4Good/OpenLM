<div align="center">
  <a href="https://huggingface.co/collections/cerebras/cerebras-gpt-66c623297a2370b8e670e0a1" target="_blank">
    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Cerebras%20GPT-ffc107?color=ffc107&logoColor=white" />
  </a>
</div>

<p align="center">
  <a href="https://arxiv.org/abs/2304.03208"><b>üìú Paper - Cerebras GPT</b></a>
</p>

---
## 1. Introdu√ß√£o

Cerebras Systems, conhecida por seus avan√ßos em hardware para IA, desenvolveu o **Cerebras GPT**, uma fam√≠lia de modelos de linguagem aberta, projetada com foco em efici√™ncia computacional. O artigo intitulado **"Cerebras-GPT: A Family of Open, Compute-Efficient Large Language Models"** detalha os fundamentos e a efici√™ncia desses modelos.

Os modelos Cerebras GPT foram treinados usando arquiteturas de alto desempenho e s√£o otimizados para execu√ß√£o eficiente em sistemas modernos.

**Destaques**:
- Primeiro modelo open-source projetado para efici√™ncia de hardware.
- Focado em reduzir o custo de treinamento e infer√™ncia sem comprometer a qualidade.
- Publicado sob a Licen√ßa Apache 2.0, permitindo personaliza√ß√£o e integra√ß√£o flex√≠vel.

---
## 2. Arquitetura do Modelo

Cerebras GPT n√£o √© um modelo √∫nico, mas sim uma fam√≠lia de modelos projetados para diferentes requisitos computacionais e aplica√ß√µes.

- **Cerebras GPT-13B**: Equil√≠brio entre desempenho e efici√™ncia computacional.
- **Cerebras GPT-6.7B**: Otimizado para tarefas gerais com menor custo.
- **Cerebras GPT-2.7B**: Destinado a dispositivos de borda com recursos limitados.
- **Cerebras GPT-111M**: Vers√£o compacta para aplica√ß√µes leves e integra√ß√£o embarcada.

Os modelos seguem a arquitetura Transformer, com otimiza√ß√µes espec√≠ficas para efici√™ncia computacional, como:
- Redu√ß√£o de par√¢metros redundantes.
- Melhor balanceamento entre capacidade de infer√™ncia e consumo de energia.

---
## 3. Treinamento

### 1. **Abordagem de Treinamento**:
Os modelos Cerebras GPT foram treinados com foco na efici√™ncia, utilizando o sistema **Cerebras Wafer-Scale Engine (WSE)**. Este hardware especializado permite:
- Redu√ß√£o do tempo de treinamento.
- Maior escalabilidade.

### 2. **Dataset**:
Os modelos foram treinados em um corpus massivo de texto, otimizados para cobrir uma ampla gama de tarefas de linguagem natural, incluindo:
- Compreens√£o de linguagem.
- Tradu√ß√£o.
- Gera√ß√£o de texto.

### 3. **T√©cnicas de Otimiza√ß√£o**:
Os modelos utilizam t√©cnicas modernas de regulariza√ß√£o e inicializa√ß√£o para:
- Reduzir o risco de overfitting.
- Melhorar a generaliza√ß√£o em tarefas n√£o vistas durante o treinamento.

---
## 4. Downloads

> "GPT" refere-se aos modelos generativos baseados na arquitetura Transformer.<br>
> "Base" indica modelos gen√©ricos para tarefas gerais.<br>
> "Small" refere-se a vers√µes compactas otimizadas para aplica√ß√µes leves.

### HuggingFace
|        Model        |                            Download                             |
|:-------------------:|----------------------------------------------------------------:|
| Cerebras GPT-13B    | [ü§ó HuggingFace](https://huggingface.co/cerebras/cerebras-gpt-13b)    |
| Cerebras GPT-6.7B   | [ü§ó HuggingFace](https://huggingface.co/cerebras/cerebras-gpt-6.7b)   |
| Cerebras GPT-2.7B   | [ü§ó HuggingFace](https://huggingface.co/cerebras/cerebras-gpt-2.7b)   |
| Cerebras GPT-111M   | [ü§ó HuggingFace](https://huggingface.co/cerebras/cerebras-gpt-111m)   |

---
## 5. Benchmarks

Resultados de benchmarks completos est√£o dispon√≠veis na [documenta√ß√£o oficial](https://cerebras.ai/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/).

**M√©tricas principais**:
- **Perplexidade**: Menor do que modelos de tamanho similar.
- **Tempo de Infer√™ncia**: Reduzido em at√© 30% devido √† arquitetura otimizadora.

---
## 6. Compara√ß√£o com Outros Modelos

**Cerebras GPT vs. DeepSeek-R1**

| M√©trica              | Cerebras GPT                      | DeepSeek-R1                           |
|-----------------------|-----------------------------------|---------------------------------------|
| Licen√ßa              | Apache 2.0                        | MIT                                   |
| Abordagem de Treinamento | Uso de WSE para efici√™ncia computacional | Aprendizado por Refor√ßo Extensivo      |
| Foco Principal        | Efici√™ncia e custo               | Racioc√≠nio L√≥gico e Resolu√ß√£o de Problemas |

---
## 7. Requisitos de Sistema

| Vers√£o do Modelo | VRAM (GPU)      | RAM (CPU)     | Armazenamento |
|-------------------|-----------------|---------------|---------------|
| Cerebras GPT-111M            | 2GB+           | 4GB+          | 2GB           |
| Cerebras GPT-2.7B            | 8GB+           | 16GB+         | 8GB           |
| Cerebras GPT-6.7B            | 16GB+          | 32GB+         | 25GB          |
| Cerebras GPT-13B             | 32GB+          | 64GB+         | 50GB          |

---
## 8. Fontes

- [Artigo Cerebras GPT](https://arxiv.org/abs/2304.03208)
- [Blog Oficial Cerebras](https://cerebras.ai/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/)
- [HuggingFace - Cerebras GPT](https://huggingface.co/collections/cerebras/cerebras-gpt-66c623297a2370b8e670e0a1)
- [Site Oficial Cerebras](https://cerebras.ai/)

