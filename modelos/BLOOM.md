# Modelo BLOOM

> **BLOOM (BigScience Large Open-science Open-access Multilingual Language Model)** √© um modelo de linguagem natural desenvolvido pela iniciativa BigScience, projetado para ser acess√≠vel, transparente e capaz de lidar com m√∫ltiplos idiomas. Ele foi treinado em colabora√ß√£o com pesquisadores de todo o mundo e apresentado como um marco em ci√™ncia aberta e colabora√ß√£o.

---
## üîç Caracter√≠sticas Principais

- **Criador:** BigScience (iniciativa liderada pela Hugging Face)
- **Arquitetura:** Transformer
- **Idiomas Suportados:** 46 idiomas e 13 linguagens de programa√ß√£o
- **Tamanho do Modelo:** 176 bilh√µes de par√¢metros (maior modelo open-access dispon√≠vel no momento do lan√ßamento)
- **Treinamento:** Realizado em um supercomputador Jean Zay (Fran√ßa), utilizando recursos de HPC (High-Performance Computing)
- **Objetivos de Treinamento:**
  - Gera√ß√£o de texto em m√∫ltiplos idiomas.
  - Apoio a diversas tarefas de NLP, como tradu√ß√£o, resumo e classifica√ß√£o.

---
## ü§ì Distribui√ß√£o das Linguagens Treinadas

![image](https://github.com/user-attachments/assets/8200135a-8e3c-44c5-ab44-e76a10c178f7)

---
## üèõÔ∏è Arquitetura

![image](https://github.com/user-attachments/assets/1f85c0ac-ab1f-49dc-ad8a-1619ad5679cd)

A arquitetura do modelo BLOOM, conforme detalhado no artigo, inclui diversos aspectos:
- A equipe se concentrou em fam√≠lias de modelos escal√°veis, com suporte em ferramentas e bases de c√≥digo dispon√≠veis publicamente.
- Experimentos de abla√ß√£o foram conduzidos em modelos menores para otimizar componentes e hiperpar√¢metros.
- A generaliza√ß√£o zero-shot foi uma m√©trica chave para avaliar as decis√µes arquiteturais.
- O BLOOM √© baseado na arquitetura Transformer, especificamente em um modelo causal *decoder-only*.
- Essa abordagem foi validada como a mais eficaz para capacidades de generaliza√ß√£o zero-shot em compara√ß√£o com arquiteturas *encoder-decoder* e outros modelos *decoder-only*.

---
## üß™ Desempenho em Benchmarks

BLOOM foi avaliado em benchmarks diversos, destacando-se em tarefas de NLP multilingu√≠stico e de programa√ß√£o. Embora n√£o alcance a performance de modelos propriet√°rios como GPT-3 em alguns casos, ele √© competitivo em cen√°rios de gera√ß√£o e compreens√£o de texto.

| **Benchmark**         | **Vers√£o Avaliada** | **Tarefa**                    | **M√©trica**         | **Resultado** |
|------------------------|---------------------|--------------------------------|---------------------|---------------|
| WMT'14 EN-FR          | BLOOM-176B         | Tradu√ß√£o de Ingl√™s para Franc√™s | BLEU Score          | 40.98%         |
| XNLI                  | BLOOM-176B         | Infer√™ncia Multil√≠ngue         | Exatid√£o M√©dia (%)  | 76.8%          |
| SuperGLUE             | BLOOM-7.1B         | Classifica√ß√£o e Infer√™ncia     | Pontua√ß√£o M√©dia (%) | 65.2%          |
| TYDI QA               | BLOOM-176B         | Perguntas e Respostas Multil√≠ngues | Exatid√£o (%)    | 58.0%          |
| LAMBADA               | BLOOM-176B         | Completar Palavras             | Exatid√£o (%)        | 77.5%          |
| MASSIVE               | BLOOM-7.1B         | Compreens√£o Multil√≠ngue        | Exatid√£o M√©dia (%)  | 48.1%          |
| WikiText103           | BLOOM-176B         | Modelagem de Linguagem         | Perplexidade        | 18.2%          |
| MLQA                  | BLOOM-176B         | Perguntas Multil√≠ngues         | Exatid√£o (%)        | 74.1%          |

> Fontes: [Hugging Face](https://huggingface.co/bigscience/bloom), [Arxiv](https://arxiv.org/abs/2211.05100)

---
## üìö Pesquisas Relevantes

- [Artigo BLOOM](https://arxiv.org/abs/2211.05100)
- [Papers with Code - BLOOM](https://paperswithcode.com/method/bloom)
- [Artigo Medium](https://medium.com/dair-ai/papers-explained-52-bloom-9654c56cd2)

---
## üì• Vers√µes

| Vers√£o            | Par√¢metros     | Idiomas | Observa√ß√µes                                      |
|-------------------|----------------|---------|-------------------------------------------------|
| **BLOOM 176B**    | 176 bilh√µes    | 46      | Modelo completo, ideal para grandes tarefas.    |
| **BLOOM 7.1B**    | 7,1 bilh√µes    | 46      | Variante menor, √∫til para experimenta√ß√£o.       |
| **BLOOM 560M**    | 560 milh√µes    | 46      | Variante compacta para aplica√ß√µes leves.        |

---
## üíª Requisitos Recomendados

| Vers√£o            | Mem√≥ria GPU        | Processador                           | Observa√ß√µes                                    |
|-------------------|--------------------|---------------------------------------|-----------------------------------------------|
| **BLOOM 176B**    | 8 GPUs (80GB HBM)  | CPUs com suporte para paralelismo     | Necess√°rio hardware robusto para execu√ß√£o.    |
| **BLOOM 7.1B**    | 2 GPUs (16GB VRAM) | CPU padr√£o com 4+ n√∫cleos             | Adequado para tarefas m√©dias de NLP.          |
| **BLOOM 560M**    | 1 GPU (8GB VRAM)   | CPU padr√£o com 2+ n√∫cleos             | Ideal para testes r√°pidos e aplica√ß√µes leves. |

---
## ‚úÖ Pr√≥s
1. **Acessibilidade:** Primeiro modelo com 176B de par√¢metros totalmente open-access.
2. **Multilinguismo Real:** Treinado para gerar texto em uma ampla gama de idiomas, incluindo l√≠nguas sub-representadas.
3. **Ci√™ncia Aberta:** Foco em transpar√™ncia, com documenta√ß√£o detalhada e permiss√µes de uso expl√≠citas.

---
## ‚ùå Contras
1. **Exig√™ncias Computacionais Altas:** A vers√£o completa requer recursos significativos, como v√°rias GPUs de alta performance.
2. **Performance em Compara√ß√£o com Modelos Propriet√°rios:** Embora seja competitivo, BLOOM fica atr√°s de modelos fechados como GPT-3 em tarefas espec√≠ficas.
3. **Custos de Treinamento:** Apesar de pr√°ticas sustent√°veis, o custo computacional para treinar o modelo foi alto.

---
## üöÄ Como Usar o BLOOM

### 1. **Instala√ß√£o**
Certifique-se de ter o Python instalado e instale a biblioteca `transformers`:
```bash
!pip install transformers -q
```

### 2. **Tokeniza√ß√£o**
```python
model = AutoModelForCausalLM.from_pretrained(model_ID, use_cache=True) 
tokenizer = AutoTokenizer.from_pretrained(model_ID)
set_seed(2024)

story_title = 'An Unexpected Journey Through Time' 
prompt = f'This is a creative story about {story_title}.\n'

input_ids = tokenizer(prompt, return_tensors="pt").to(0)

sample = model.generate(**input_ids, 
                        max_length=200, top_k=1, 
                        temperature=0, repetition_penalty=2.0)

generated_story = tokenizer.decode(sample[0], skip_special_tokens=True)

import textwrap  
wrapper = textwrap.TextWrapper(width=80)

formated_story = wrapper.fill(text=generated_story)
print(formated_story)
```

---
## üìú Fontes

- [Hugging Face: BLOOM](https://huggingface.co/bigscience/bloom)
- [Blog BigScience](https://bigscience.huggingface.co/blog/bloom)
- [Wikipedia: BLOOM](https://en.wikipedia.org/wiki/BLOOM_(language_model))
- [Exploring BLOOM - DataCamp](https://www.datacamp.com/blog/exploring-bloom-guide-to-multilingual-llm)
- [Artigo Acad√™mico no Arxiv](https://arxiv.org/abs/2211.05100)
- [Artigo no Medium: Cobus Greyling](https://cobusgreyling.medium.com/bloom-bigscience-large-open-science-open-access-multilingual-language-model-b45825aa119e)
- [AMD Infinity Hub](https://www.amd.com/pt/developer/resources/infinity-hub/bloom-176b.html)
- [Admantium Blog](https://admantium.com/blog/llm04_gen2_gen3_overview_part2/)
